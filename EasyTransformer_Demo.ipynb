{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/neelnanda-io/Easy-Transformer/blob/demo_notebook/EasyTransformer_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running as a Jupyter notebook - intended for development only!\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  import google.colab\n",
    "  IN_COLAB = True\n",
    "  print(\"Running as a Colab notebook\")\n",
    "except:\n",
    "  IN_COLAB = False\n",
    "  print(\"Running as a Jupyter notebook - intended for development only!\")\n",
    "  from IPython import get_ipython\n",
    "  ipython = get_ipython()\n",
    "  # Code to automatically update the EasyTransformer code as its edited without restarting the kernel\n",
    "  ipython.magic(\"load_ext autoreload\")\n",
    "  ipython.magic(\"autoreload 2\")\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "if IN_COLAB:\n",
    "    os.system('pip install git+https://github.com/neelnanda-io/Easy-Transformer.git')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Import stuff\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import einops\n",
    "import tqdm.notebook as tqdm\n",
    "\n",
    "import random\n",
    "import time\n",
    "\n",
    "# from google.colab import drive\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"colab\"\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from functools import *\n",
    "import pandas as pd\n",
    "import gc\n",
    "import collections\n",
    "import copy\n",
    "\n",
    "# import comet_ml\n",
    "import itertools\n",
    "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n",
    "import dataclasses\n",
    "import datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from easy_transformer.utils import gelu_new, to_numpy, get_corner #helper functions\n",
    "from easy_transformer.hook_points import HookedRootModule, HookPoint\n",
    "from easy_transformer.EasyTransformer import EasyTransformer,TransformerBlock, MLP, Attention, LayerNormPre, PosEmbed, Unembed, Embed\n",
    "from easy_transformer.experiments import ExperimentMetric, AblationConfig, EasyAblation, EasyPatching, PatchingConfig\n",
    "from easy_transformer.EasyTransformerConfig import EasyTransformerConfig\n",
    "import easy_transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Hook Points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Garcon-style interface - the key thing is a HookPoint class. This is a layer to wrap any activation within the model in. The HookPoint acts as an identity function, but allows us to put PyTorch hooks in to edit and access the relevant activation. This allows us to take any model and insert in access points to all interesting activations by wrapping them in HookPoints\n",
    "\n",
    "There is also a `HookedRootModule` class - this is a utility class that the root module should inherit from (root module = the model we run) - it has several utility functions for using hooks well. \n",
    "\n",
    "The default interface is the `run_with_hooks` function on the root module, which lets us run a forwards pass on the model, and pass on a list of hooks paired with layer names to run on that pass. \n",
    "\n",
    "The syntax for a hook is `function(activation, hook)` where `activation` is the activation the hook is wrapped around, and `hook` is the `HookPoint` class the function is attached to. If the function returns a new activation or edits the activation in-place, that replaces the old one, if it returns None then the activation remains as is.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hook Points Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a simple example of how to use the classes:\n",
    "\n",
    "We define a basic network with two layers that each take a scalar input $x$, square it, and add a constant:\n",
    "$x_0=x$, $x_1=x_0^2+3$, $x_2=x_1^2-4$.\n",
    "\n",
    "We wrap the input, each layer's output, and the intermediate value of each layer (the square) in a hook point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from easy_transformer.hook_points import HookedRootModule, HookPoint\n",
    "\n",
    "class SquareThenAdd(nn.Module):\n",
    "    def __init__(self, offset):\n",
    "        super().__init__()\n",
    "        self.offset = nn.Parameter(torch.tensor(offset))\n",
    "        self.hook_square = HookPoint()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # The hook_square doesn't change the value, but lets us access it\n",
    "        square = self.hook_square(x * x)\n",
    "        return self.offset + square\n",
    "    \n",
    "class TwoLayerModel(HookedRootModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = SquareThenAdd(3.)\n",
    "        self.layer2 = SquareThenAdd(-4.)\n",
    "        self.hook_in = HookPoint()\n",
    "        self.hook_mid = HookPoint()\n",
    "        self.hook_out = HookPoint()\n",
    "\n",
    "        # We need to call the setup function of HookedRootModule to build an \n",
    "        # internal dictionary of modules and hooks, and to give each hook a name\n",
    "        super().setup()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # We wrap the input and each layer's output in a hook - they leave the \n",
    "        # value unchanged (unless there's a hook added to explicitly change it), \n",
    "        # but allow us to access it.\n",
    "        x_in = self.hook_in(x)\n",
    "        x_mid = self.hook_mid(self.layer1(x_in))\n",
    "        x_out = self.hook_out(self.layer2(x_mid))\n",
    "        return x_out\n",
    "model = TwoLayerModel()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add a cache, to save the activation at each hook point\n",
    "\n",
    "(There's a custom `cache_all` function on the root module as a convenience, which will add hooks to cache every activation at a hook point - we could also manually add hooks with `run_with_hooks`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "cache = {}\n",
    "model.cache_all(cache)\n",
    "print('Model output:', model(torch.tensor(5.)).item())\n",
    "for key in cache:\n",
    "    print(f\"Value cached at hook {key}\", cache[key].item())\n",
    "model.reset_hooks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use hooks to intervene on activations - eg, we can set the intermediate value in layer 2 to zero to change the output to -5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def set_to_zero_hook(tensor, hook):\n",
    "    print(hook.name)\n",
    "    return torch.tensor(0.)\n",
    "print('Output after intervening on layer2.hook_scaled', \n",
    "      model.run_with_hooks(torch.tensor(5.),\n",
    "                           fwd_hooks = [('layer2.hook_square', set_to_zero_hook)]).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define a stripped down transformer. There are helper functions to load in the weights of several families of open source LLMs - OpenAI's GPT-2, Facebook's OPT and Eleuther's GPT-Neo.\n",
    "\n",
    "Note: OPT-350M is not supported - it applies the LayerNorms to the *outputs* of each layer, which means we cannot fold the weights and biases into other layers, and would require notably different architecture.\n",
    "\n",
    "**TODO:** Add in GPT-J and GPT-NeoX functionality\n",
    "\n",
    "The list of supported model names:\n",
    "```\n",
    " ['gpt2', \n",
    "                     'gpt2-medium', \n",
    "                     'gpt2-large', \n",
    "                     'gpt2-xl', \n",
    "                     'facebook/opt-125m', \n",
    "                     'facebook/opt-1.3b', \n",
    "                     'facebook/opt-2.7b', \n",
    "                     'facebook/opt-6.7b', \n",
    "                     'facebook/opt-13b', \n",
    "                     'facebook/opt-30b', \n",
    "                     'facebook/opt-66b', \n",
    "                     'EleutherAI/gpt-neo-125M', \n",
    "                     'EleutherAI/gpt-neo-1.3B', \n",
    "                     'EleutherAI/gpt-neo-2.7B', \n",
    "                     'EleutherAI/gpt-j-6B', \n",
    "                     'EleutherAI/gpt-neox-20b']\n",
    "                     ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in GPT-2 small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "model_name = 'gpt2' #@param ['gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl', 'facebook/opt-125m', 'facebook/opt-1.3b', 'facebook/opt-2.7b', 'facebook/opt-6.7b', 'facebook/opt-13b', 'facebook/opt-30b', 'facebook/opt-66b', 'EleutherAI/gpt-neo-125M', 'EleutherAI/gpt-neo-1.3B', 'EleutherAI/gpt-neo-2.7B', 'EleutherAI/gpt-j-6B', 'EleutherAI/gpt-neox-20b']\n",
    "model = EasyTransformer.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create some reference text to run the models on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inter|pret|ability| is| great\n",
      "AI| Al|ignment| is| great\n"
     ]
    }
   ],
   "source": [
    "prompt = 'Interpretability is great'\n",
    "tokens = model.to_tokens(prompt)\n",
    "prompt_2 = 'AI Alignment is great'\n",
    "tokens_2 = model.to_tokens(prompt_2)\n",
    "def show_tokens(tokens):\n",
    "    # Prints the tokens as text, separated by |\n",
    "    if type(tokens)==str:\n",
    "        # If we input text, tokenize first\n",
    "        tokens = model.to_tokens(tokens)\n",
    "    text_tokens = [model.tokenizer.decode(t) for t in tokens.squeeze()]\n",
    "    print('|'.join(text_tokens))\n",
    "show_tokens(tokens)\n",
    "show_tokens(tokens_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can also do some standard generation/sampling stuff - currently only supports sampling (with various modifiers) and greedy decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:03<00:00, 12.51it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[\"Interpretability is great. It's not a bad thing to have an idea of what you're doing, but it can be very confusing if the reader isn't familiar with your work and how they feel about it.\\n\\nThe best way to make sure that readers understand\",\n",
       " \"AI Alignment is great. I love it!\\n\\nI am a huge fan of the new version, but this one seems to be more like an update on my previous build (which was about 6 months ago). The main difference between these two versions are that there's\"]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.tokenizer.batch_decode(model.generate([prompt, prompt_2], max_new_tokens=50, do_sample=True, temperature=0.6, top_k=50, top_p=0.7, freq_penalty=5, use_cache=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 12.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top corner of logits\n",
      "tensor([[[-5.0197, -4.0007, -6.4540, -6.6005],\n",
      "         [-4.1477, -2.2966, -7.4325, -6.9754],\n",
      "         [-2.7587,  1.3903, -4.3042, -6.5975],\n",
      "         [-6.1653, -5.0678, -9.1324, -9.5672]]], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model.reset_hooks()\n",
    "original_logits = model(tokens)\n",
    "print('Top corner of logits')\n",
    "print(get_corner(original_logits, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference: Hyperparameters for the model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'d_model': 768,\n",
       " 'd_head': 64,\n",
       " 'n_heads': 12,\n",
       " 'd_mlp': 3072,\n",
       " 'n_layers': 12,\n",
       " 'n_ctx': 1024,\n",
       " 'd_vocab': 50257,\n",
       " 'act_fn': 'gelu_new',\n",
       " 'eps': 1e-05,\n",
       " 'use_attn_result': False,\n",
       " 'use_attn_scale': True,\n",
       " 'use_local_attn': False,\n",
       " 'model_name': 'gpt2',\n",
       " 'model_type': 'gpt2',\n",
       " 'checkpoint': None,\n",
       " 'full_model_name': None,\n",
       " 'tokenizer_name': 'gpt2',\n",
       " 'window_size': None,\n",
       " 'attn_types': None,\n",
       " 'init_mode': 'gpt2',\n",
       " 'normalization_type': 'LNPre',\n",
       " 'gated_act_fn': False}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Reference: Hyperparameters for the model')\n",
    "dataclasses.asdict(model.cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Using the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model can be given either text or tokens as an input (text is automatically converted to a `batch_size=1` batch of tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "logits_tokens = model(tokens)\n",
    "logits_text = model(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model gives the same log_probs as the original Hugging Face model \n",
    "\n",
    "Though *not* the same logits, as we remove a constant offset from $W_U$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of log probs the same between easy model and original model:\n",
      "tensor(1.0000)\n",
      "Fraction of logits the same between easy model and original model:\n",
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "original_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "easy_logits = model(tokens).cpu()\n",
    "original_model_logits = original_model(tokens).logits\n",
    "\n",
    "easy_log_probs = F.log_softmax(easy_logits, dim=-1)\n",
    "original_model_log_probs = F.log_softmax(original_model_logits, dim=-1)\n",
    "\n",
    "print('Fraction of log probs the same between easy model and original model:')\n",
    "print(torch.isclose(original_model_log_probs, easy_log_probs).sum()/easy_log_probs.numel())\n",
    "print('Fraction of logits the same between easy model and original model:')\n",
    "print(torch.isclose(original_model_logits, easy_logits).sum()/easy_logits.numel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Basic Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the shapes of all activations\n",
    "\n",
    "**Note:** This cell is a good reference for creating hooks - it's extremely useful to know the shapes of different activations as accessible by each hook!\n",
    "\n",
    "By convention, each activation is batch x position x ... (where the final dimension(s) is d_model, (head_index x d_head) or d_mlp). The one exception is hook_attn (attention patterns) which has shape batch x head_index x query_pos x key_pos\n",
    "\n",
    "**Reference:**\n",
    "`batch_size=4\n",
    "n_ctx=50\n",
    "d_head=64\n",
    "d_model=768\n",
    "d_mlp=3072\n",
    "n_heads=12\n",
    "n_layers=12`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation at hook hook_embed has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook hook_pos_embed has shape:\n",
      "torch.Size([50, 768])\n",
      "Activation at hook blocks.0.hook_resid_pre has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook blocks.0.ln1.hook_scale has shape:\n",
      "torch.Size([4, 50, 1])\n",
      "Activation at hook blocks.0.ln1.hook_normalized has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook blocks.0.attn.hook_q has shape:\n",
      "torch.Size([4, 50, 12, 64])\n",
      "Activation at hook blocks.0.attn.hook_k has shape:\n",
      "torch.Size([4, 50, 12, 64])\n",
      "Activation at hook blocks.0.attn.hook_v has shape:\n",
      "torch.Size([4, 50, 12, 64])\n",
      "Activation at hook blocks.0.attn.hook_attn_scores has shape:\n",
      "torch.Size([4, 12, 50, 50])\n",
      "Activation at hook blocks.0.attn.hook_attn has shape:\n",
      "torch.Size([4, 12, 50, 50])\n",
      "Activation at hook blocks.0.attn.hook_z has shape:\n",
      "torch.Size([4, 50, 12, 64])\n",
      "Activation at hook blocks.0.hook_attn_out has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook blocks.0.hook_resid_mid has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook blocks.0.ln2.hook_scale has shape:\n",
      "torch.Size([4, 50, 1])\n",
      "Activation at hook blocks.0.ln2.hook_normalized has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook blocks.0.mlp.hook_pre has shape:\n",
      "torch.Size([4, 50, 3072])\n",
      "Activation at hook blocks.0.mlp.hook_post has shape:\n",
      "torch.Size([4, 50, 3072])\n",
      "Activation at hook blocks.0.hook_mlp_out has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook blocks.0.hook_resid_post has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook blocks.1.hook_resid_pre has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook blocks.1.ln1.hook_scale has shape:\n",
      "torch.Size([4, 50, 1])\n",
      "Activation at hook blocks.1.ln1.hook_normalized has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook blocks.1.attn.hook_q has shape:\n",
      "torch.Size([4, 50, 12, 64])\n",
      "Activation at hook blocks.1.attn.hook_k has shape:\n",
      "torch.Size([4, 50, 12, 64])\n",
      "Activation at hook blocks.1.attn.hook_v has shape:\n",
      "torch.Size([4, 50, 12, 64])\n",
      "Activation at hook blocks.1.attn.hook_attn_scores has shape:\n",
      "torch.Size([4, 12, 50, 50])\n",
      "Activation at hook blocks.1.attn.hook_attn has shape:\n",
      "torch.Size([4, 12, 50, 50])\n",
      "Activation at hook blocks.1.attn.hook_z has shape:\n",
      "torch.Size([4, 50, 12, 64])\n",
      "Activation at hook blocks.1.hook_attn_out has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook blocks.1.hook_resid_mid has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook blocks.1.ln2.hook_scale has shape:\n",
      "torch.Size([4, 50, 1])\n",
      "Activation at hook blocks.1.ln2.hook_normalized has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook blocks.1.mlp.hook_pre has shape:\n",
      "torch.Size([4, 50, 3072])\n",
      "Activation at hook blocks.1.mlp.hook_post has shape:\n",
      "torch.Size([4, 50, 3072])\n",
      "Activation at hook blocks.1.hook_mlp_out has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook blocks.1.hook_resid_post has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook blocks.2.hook_resid_pre has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook blocks.2.ln1.hook_scale has shape:\n",
      "torch.Size([4, 50, 1])\n",
      "Activation at hook blocks.2.ln1.hook_normalized has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook blocks.2.attn.hook_q has shape:\n",
      "torch.Size([4, 50, 12, 64])\n",
      "Activation at hook blocks.2.attn.hook_k has shape:\n",
      "torch.Size([4, 50, 12, 64])\n",
      "Activation at hook blocks.2.attn.hook_v has shape:\n",
      "torch.Size([4, 50, 12, 64])\n",
      "Activation at hook blocks.2.attn.hook_attn_scores has shape:\n",
      "torch.Size([4, 12, 50, 50])\n",
      "Activation at hook blocks.2.attn.hook_attn has shape:\n",
      "torch.Size([4, 12, 50, 50])\n",
      "Activation at hook blocks.2.attn.hook_z has shape:\n",
      "torch.Size([4, 50, 12, 64])\n",
      "Activation at hook blocks.2.hook_attn_out has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook blocks.2.hook_resid_mid has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook blocks.2.ln2.hook_scale has shape:\n",
      "torch.Size([4, 50, 1])\n",
      "Activation at hook blocks.2.ln2.hook_normalized has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook blocks.2.mlp.hook_pre has shape:\n",
      "torch.Size([4, 50, 3072])\n",
      "Activation at hook blocks.2.mlp.hook_post has shape:\n",
      "torch.Size([4, 50, 3072])\n",
      "Activation at hook blocks.2.hook_mlp_out has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook blocks.2.hook_resid_post has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook blocks.3.hook_resid_pre has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook blocks.3.ln1.hook_scale has shape:\n",
      "torch.Size([4, 50, 1])\n",
      "Activation at hook blocks.3.ln1.hook_normalized has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook blocks.3.attn.hook_q has shape:\n",
      "torch.Size([4, 50, 12, 64])\n",
      "Activation at hook blocks.3.attn.hook_k has shape:\n",
      "torch.Size([4, 50, 12, 64])\n",
      "Activation at hook blocks.3.attn.hook_v has shape:\n",
      "torch.Size([4, 50, 12, 64])\n",
      "Activation at hook blocks.3.attn.hook_attn_scores has shape:\n",
      "torch.Size([4, 12, 50, 50])\n",
      "Activation at hook blocks.3.attn.hook_attn has shape:\n",
      "torch.Size([4, 12, 50, 50])\n",
      "Activation at hook blocks.3.attn.hook_z has shape:\n",
      "torch.Size([4, 50, 12, 64])\n",
      "Activation at hook blocks.3.hook_attn_out has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook blocks.3.hook_resid_mid has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook blocks.3.ln2.hook_scale has shape:\n",
      "torch.Size([4, 50, 1])\n",
      "Activation at hook blocks.3.ln2.hook_normalized has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook blocks.3.mlp.hook_pre has shape:\n",
      "torch.Size([4, 50, 3072])\n",
      "Activation at hook blocks.3.mlp.hook_post has shape:\n",
      "torch.Size([4, 50, 3072])\n",
      "Activation at hook blocks.3.hook_mlp_out has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook blocks.3.hook_resid_post has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook blocks.4.hook_resid_pre has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook blocks.4.ln1.hook_scale has shape:\n",
      "torch.Size([4, 50, 1])\n",
      "Activation at hook blocks.4.ln1.hook_normalized has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook blocks.4.attn.hook_q has shape:\n",
      "torch.Size([4, 50, 12, 64])\n",
      "Activation at hook blocks.4.attn.hook_k has shape:\n",
      "torch.Size([4, 50, 12, 64])\n",
      "Activation at hook blocks.4.attn.hook_v has shape:\n",
      "torch.Size([4, 50, 12, 64])\n",
      "Activation at hook blocks.4.attn.hook_attn_scores has shape:\n",
      "torch.Size([4, 12, 50, 50])\n",
      "Activation at hook blocks.4.attn.hook_attn has shape:\n",
      "torch.Size([4, 12, 50, 50])\n",
      "Activation at hook blocks.4.attn.hook_z has shape:\n",
      "torch.Size([4, 50, 12, 64])\n",
      "Activation at hook blocks.4.hook_attn_out has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook blocks.4.hook_resid_mid has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook blocks.4.ln2.hook_scale has shape:\n",
      "torch.Size([4, 50, 1])\n",
      "Activation at hook blocks.4.ln2.hook_normalized has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook blocks.4.mlp.hook_pre has shape:\n",
      "torch.Size([4, 50, 3072])\n",
      "Activation at hook blocks.4.mlp.hook_post has shape:\n",
      "torch.Size([4, 50, 3072])\n",
      "Activation at hook blocks.4.hook_mlp_out has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook blocks.4.hook_resid_post has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook blocks.5.hook_resid_pre has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook blocks.5.ln1.hook_scale has shape:\n",
      "torch.Size([4, 50, 1])\n",
      "Activation at hook blocks.5.ln1.hook_normalized has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook blocks.5.attn.hook_q has shape:\n",
      "torch.Size([4, 50, 12, 64])\n",
      "Activation at hook blocks.5.attn.hook_k has shape:\n",
      "torch.Size([4, 50, 12, 64])\n",
      "Activation at hook blocks.5.attn.hook_v has shape:\n",
      "torch.Size([4, 50, 12, 64])\n",
      "Activation at hook blocks.5.attn.hook_attn_scores has shape:\n",
      "torch.Size([4, 12, 50, 50])\n",
      "Activation at hook blocks.5.attn.hook_attn has shape:\n",
      "torch.Size([4, 12, 50, 50])\n",
      "Activation at hook blocks.5.attn.hook_z has shape:\n",
      "torch.Size([4, 50, 12, 64])\n",
      "Activation at hook blocks.5.hook_attn_out has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook blocks.5.hook_resid_mid has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook blocks.5.ln2.hook_scale has shape:\n",
      "torch.Size([4, 50, 1])\n",
      "Activation at hook blocks.5.ln2.hook_normalized has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook blocks.5.mlp.hook_pre has shape:\n",
      "torch.Size([4, 50, 3072])\n",
      "Activation at hook blocks.5.mlp.hook_post has shape:\n",
      "torch.Size([4, 50, 3072])\n",
      "Activation at hook blocks.5.hook_mlp_out has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook blocks.5.hook_resid_post has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook blocks.6.hook_resid_pre has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook blocks.6.ln1.hook_scale has shape:\n",
      "torch.Size([4, 50, 1])\n",
      "Activation at hook blocks.6.ln1.hook_normalized has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook blocks.6.attn.hook_q has shape:\n",
      "torch.Size([4, 50, 12, 64])\n",
      "Activation at hook blocks.6.attn.hook_k has shape:\n",
      "torch.Size([4, 50, 12, 64])\n",
      "Activation at hook blocks.6.attn.hook_v has shape:\n",
      "torch.Size([4, 50, 12, 64])\n",
      "Activation at hook blocks.6.attn.hook_attn_scores has shape:\n",
      "torch.Size([4, 12, 50, 50])\n",
      "Activation at hook blocks.6.attn.hook_attn has shape:\n",
      "torch.Size([4, 12, 50, 50])\n",
      "Activation at hook blocks.6.attn.hook_z has shape:\n",
      "torch.Size([4, 50, 12, 64])\n",
      "Activation at hook blocks.6.hook_attn_out has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook blocks.6.hook_resid_mid has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook blocks.6.ln2.hook_scale has shape:\n",
      "torch.Size([4, 50, 1])\n",
      "Activation at hook blocks.6.ln2.hook_normalized has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook blocks.6.mlp.hook_pre has shape:\n",
      "torch.Size([4, 50, 3072])\n",
      "Activation at hook blocks.6.mlp.hook_post has shape:\n",
      "torch.Size([4, 50, 3072])\n",
      "Activation at hook blocks.6.hook_mlp_out has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook blocks.6.hook_resid_post has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook blocks.7.hook_resid_pre has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook blocks.7.ln1.hook_scale has shape:\n",
      "torch.Size([4, 50, 1])\n",
      "Activation at hook blocks.7.ln1.hook_normalized has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook blocks.7.attn.hook_q has shape:\n",
      "torch.Size([4, 50, 12, 64])\n",
      "Activation at hook blocks.7.attn.hook_k has shape:\n",
      "torch.Size([4, 50, 12, 64])\n",
      "Activation at hook blocks.7.attn.hook_v has shape:\n",
      "torch.Size([4, 50, 12, 64])\n",
      "Activation at hook blocks.7.attn.hook_attn_scores has shape:\n",
      "torch.Size([4, 12, 50, 50])\n",
      "Activation at hook blocks.7.attn.hook_attn has shape:\n",
      "torch.Size([4, 12, 50, 50])\n",
      "Activation at hook blocks.7.attn.hook_z has shape:\n",
      "torch.Size([4, 50, 12, 64])\n",
      "Activation at hook blocks.7.hook_attn_out has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook blocks.7.hook_resid_mid has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook blocks.7.ln2.hook_scale has shape:\n",
      "torch.Size([4, 50, 1])\n",
      "Activation at hook blocks.7.ln2.hook_normalized has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook blocks.7.mlp.hook_pre has shape:\n",
      "torch.Size([4, 50, 3072])\n",
      "Activation at hook blocks.7.mlp.hook_post has shape:\n",
      "torch.Size([4, 50, 3072])\n",
      "Activation at hook blocks.7.hook_mlp_out has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook blocks.7.hook_resid_post has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook blocks.8.hook_resid_pre has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook blocks.8.ln1.hook_scale has shape:\n",
      "torch.Size([4, 50, 1])\n",
      "Activation at hook blocks.8.ln1.hook_normalized has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook blocks.8.attn.hook_q has shape:\n",
      "torch.Size([4, 50, 12, 64])\n",
      "Activation at hook blocks.8.attn.hook_k has shape:\n",
      "torch.Size([4, 50, 12, 64])\n",
      "Activation at hook blocks.8.attn.hook_v has shape:\n",
      "torch.Size([4, 50, 12, 64])\n",
      "Activation at hook blocks.8.attn.hook_attn_scores has shape:\n",
      "torch.Size([4, 12, 50, 50])\n",
      "Activation at hook blocks.8.attn.hook_attn has shape:\n",
      "torch.Size([4, 12, 50, 50])\n",
      "Activation at hook blocks.8.attn.hook_z has shape:\n",
      "torch.Size([4, 50, 12, 64])\n",
      "Activation at hook blocks.8.hook_attn_out has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook blocks.8.hook_resid_mid has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook blocks.8.ln2.hook_scale has shape:\n",
      "torch.Size([4, 50, 1])\n",
      "Activation at hook blocks.8.ln2.hook_normalized has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook blocks.8.mlp.hook_pre has shape:\n",
      "torch.Size([4, 50, 3072])\n",
      "Activation at hook blocks.8.mlp.hook_post has shape:\n",
      "torch.Size([4, 50, 3072])\n",
      "Activation at hook blocks.8.hook_mlp_out has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook blocks.8.hook_resid_post has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook blocks.9.hook_resid_pre has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook blocks.9.ln1.hook_scale has shape:\n",
      "torch.Size([4, 50, 1])\n",
      "Activation at hook blocks.9.ln1.hook_normalized has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook blocks.9.attn.hook_q has shape:\n",
      "torch.Size([4, 50, 12, 64])\n",
      "Activation at hook blocks.9.attn.hook_k has shape:\n",
      "torch.Size([4, 50, 12, 64])\n",
      "Activation at hook blocks.9.attn.hook_v has shape:\n",
      "torch.Size([4, 50, 12, 64])\n",
      "Activation at hook blocks.9.attn.hook_attn_scores has shape:\n",
      "torch.Size([4, 12, 50, 50])\n",
      "Activation at hook blocks.9.attn.hook_attn has shape:\n",
      "torch.Size([4, 12, 50, 50])\n",
      "Activation at hook blocks.9.attn.hook_z has shape:\n",
      "torch.Size([4, 50, 12, 64])\n",
      "Activation at hook blocks.9.hook_attn_out has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook blocks.9.hook_resid_mid has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook blocks.9.ln2.hook_scale has shape:\n",
      "torch.Size([4, 50, 1])\n",
      "Activation at hook blocks.9.ln2.hook_normalized has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook blocks.9.mlp.hook_pre has shape:\n",
      "torch.Size([4, 50, 3072])\n",
      "Activation at hook blocks.9.mlp.hook_post has shape:\n",
      "torch.Size([4, 50, 3072])\n",
      "Activation at hook blocks.9.hook_mlp_out has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook blocks.9.hook_resid_post has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook blocks.10.hook_resid_pre has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook blocks.10.ln1.hook_scale has shape:\n",
      "torch.Size([4, 50, 1])\n",
      "Activation at hook blocks.10.ln1.hook_normalized has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook blocks.10.attn.hook_q has shape:\n",
      "torch.Size([4, 50, 12, 64])\n",
      "Activation at hook blocks.10.attn.hook_k has shape:\n",
      "torch.Size([4, 50, 12, 64])\n",
      "Activation at hook blocks.10.attn.hook_v has shape:\n",
      "torch.Size([4, 50, 12, 64])\n",
      "Activation at hook blocks.10.attn.hook_attn_scores has shape:\n",
      "torch.Size([4, 12, 50, 50])\n",
      "Activation at hook blocks.10.attn.hook_attn has shape:\n",
      "torch.Size([4, 12, 50, 50])\n",
      "Activation at hook blocks.10.attn.hook_z has shape:\n",
      "torch.Size([4, 50, 12, 64])\n",
      "Activation at hook blocks.10.hook_attn_out has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook blocks.10.hook_resid_mid has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook blocks.10.ln2.hook_scale has shape:\n",
      "torch.Size([4, 50, 1])\n",
      "Activation at hook blocks.10.ln2.hook_normalized has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook blocks.10.mlp.hook_pre has shape:\n",
      "torch.Size([4, 50, 3072])\n",
      "Activation at hook blocks.10.mlp.hook_post has shape:\n",
      "torch.Size([4, 50, 3072])\n",
      "Activation at hook blocks.10.hook_mlp_out has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook blocks.10.hook_resid_post has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook blocks.11.hook_resid_pre has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook blocks.11.ln1.hook_scale has shape:\n",
      "torch.Size([4, 50, 1])\n",
      "Activation at hook blocks.11.ln1.hook_normalized has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook blocks.11.attn.hook_q has shape:\n",
      "torch.Size([4, 50, 12, 64])\n",
      "Activation at hook blocks.11.attn.hook_k has shape:\n",
      "torch.Size([4, 50, 12, 64])\n",
      "Activation at hook blocks.11.attn.hook_v has shape:\n",
      "torch.Size([4, 50, 12, 64])\n",
      "Activation at hook blocks.11.attn.hook_attn_scores has shape:\n",
      "torch.Size([4, 12, 50, 50])\n",
      "Activation at hook blocks.11.attn.hook_attn has shape:\n",
      "torch.Size([4, 12, 50, 50])\n",
      "Activation at hook blocks.11.attn.hook_z has shape:\n",
      "torch.Size([4, 50, 12, 64])\n",
      "Activation at hook blocks.11.hook_attn_out has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook blocks.11.hook_resid_mid has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook blocks.11.ln2.hook_scale has shape:\n",
      "torch.Size([4, 50, 1])\n",
      "Activation at hook blocks.11.ln2.hook_normalized has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook blocks.11.mlp.hook_pre has shape:\n",
      "torch.Size([4, 50, 3072])\n",
      "Activation at hook blocks.11.mlp.hook_post has shape:\n",
      "torch.Size([4, 50, 3072])\n",
      "Activation at hook blocks.11.hook_mlp_out has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook blocks.11.hook_resid_post has shape:\n",
      "torch.Size([4, 50, 768])\n",
      "Activation at hook ln_final.hook_scale has shape:\n",
      "torch.Size([4, 50, 1])\n",
      "Activation at hook ln_final.hook_normalized has shape:\n",
      "torch.Size([4, 50, 768])\n"
     ]
    }
   ],
   "source": [
    "all_hooks_fn = lambda name: True\n",
    "def print_shape(tensor, hook):\n",
    "    print(f'Activation at hook {hook.name} has shape:')\n",
    "    print(tensor.shape)\n",
    "random_tokens = torch.randint(1000, 10000, (4, 50))\n",
    "logits = model.run_with_hooks(random_tokens, fwd_hooks=[(all_hooks_fn, print_shape)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the top corner of all activations\n",
    "\n",
    "**Note:** This is useful to do as a sanity check when debugging a model, to quickly and roughly compare the new activations to the original activations (without looking at the full enormous tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def print_corner(tensor, hook):\n",
    "    print(hook.name)\n",
    "    print(get_corner(tensor))\n",
    "logits = model.run_with_hooks(tokens, fwd_hooks=[(all_hooks_fn, print_corner)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cache all activations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "cache = {}\n",
    "model.reset_hooks()\n",
    "model.cache_all(cache)\n",
    "logits = model(tokens)\n",
    "for name in cache:\n",
    "    print(name, cache[name].shape)\n",
    "model.reset_hooks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save GPU memory, we can cache activations to the CPU - note that this is much slower though, since it requires copying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "random_tokens = torch.randint(1000, 10000, (1, 300))\n",
    "cache = {}\n",
    "model.reset_hooks()\n",
    "model.cache_all(cache, device='cpu')\n",
    "print('Run time when copying to the CPU')\n",
    "%timeit logits = model(random_tokens)\n",
    "model.reset_hooks()\n",
    "model.cache_all(cache, device='cuda')\n",
    "print('Run time when just caching on GPU')\n",
    "%timeit logits = model(random_tokens)\n",
    "model.reset_hooks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Editing Activations\n",
    "**To change an activation, add a hook to that HookPoint which returns the new activation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pruning attention heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Example - prune heads 0, 3 and 7 from layer 3 and heads 8 and 9 from layer 7\n",
    "layer = 3\n",
    "head_indices = torch.tensor([0, 3, 7])\n",
    "layer_2 = 7\n",
    "head_indices_2 = torch.tensor([8, 9])\n",
    "def prune_fn_1(z, hook):\n",
    "    # The shape of the z tensor is batch x pos x head_index x d_head\n",
    "    z[:, :, head_indices, :] = 0.\n",
    "    return z\n",
    "def prune_fn_2(z, hook):\n",
    "    # The shape of the z tensor is batch x pos x head_index x d_head\n",
    "    z[:, :, head_indices_2, :] = 0.\n",
    "    return z\n",
    "logits = model.run_with_hooks(tokens, fwd_hooks=[(f'blocks.{layer}.attn.hook_z', prune_fn_1),\n",
    "                                                       (f'blocks.{layer_2}.attn.hook_z', prune_fn_2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restrict all attention heads to only attend to the current and previous token.\n",
    "\n",
    "**Validation:** The logits for the first 2 positions are the same, the logits for pos 3 are different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "model.reset_hooks()\n",
    "def filter_hook_attn(name):\n",
    "    split_name = name.split('.')\n",
    "    return (split_name[-1]=='hook_attn')\n",
    "def restrict_attn(attn, hook):\n",
    "    # Attn has shape batch x head_index x query_pos x key_pos\n",
    "    n_ctx = attn.size(-2)\n",
    "    key_pos = torch.arange(n_ctx)[None, :]\n",
    "    query_pos = torch.arange(n_ctx)[:, None]\n",
    "    mask = (key_pos>(query_pos-2)).cuda()\n",
    "    ZERO = torch.tensor(0.)\n",
    "    if torch.cuda.is_available():\n",
    "        ZERO = ZERO.cuda()\n",
    "    attn = torch.where(mask, attn, ZERO)\n",
    "    return attn\n",
    "logits = model.run_with_hooks(tokens, fwd_hooks=[(filter_hook_attn, restrict_attn)])\n",
    "print('New logits')\n",
    "print(get_corner(logits, 3))\n",
    "print('Original logits')\n",
    "print(get_corner(original_logits, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Freezing attention patterns - here we do two runs of the model. First on the original text, caching attn patterns, and secondly on the new text, loading the cached patterns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "attn_cache = {}\n",
    "def cache_attn(attn, hook):\n",
    "    attn_cache[hook.name]=attn\n",
    "\n",
    "def freeze_attn(attn, hook):\n",
    "    return attn_cache[hook.name]\n",
    "\n",
    "logits = model.run_with_hooks(tokens, fwd_hooks=[(filter_hook_attn, cache_attn)])\n",
    "\n",
    "logits_2 = model.run_with_hooks(tokens_2, fwd_hooks=[(filter_hook_attn, freeze_attn)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Using Hook Contexts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Each hook point has a dictionary `hook.ctx` that can be used to store information between runs** - this is useful for keeping running totals, etc "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A running total of times a neuron activation was positive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# We focus on neuron 20 in layer 7\n",
    "model.reset_hooks()\n",
    "animal_texts = ['The dog was green', 'The cat was blue', 'The squid was magenta', 'The blobfish was grey']\n",
    "layer = 7\n",
    "neuron_index = 20\n",
    "def running_total_hook(neuron_acts, hook):\n",
    "    if 'total' not in hook.ctx:\n",
    "        hook.ctx['total']=0\n",
    "    print('Neuron acts:', neuron_acts[0, :, neuron_index])\n",
    "    hook.ctx['total']+=(neuron_acts[0, :, neuron_index]>0).sum().item()\n",
    "    print('Running total:', hook.ctx['total'])\n",
    "\n",
    "for animal_text in animal_texts:\n",
    "    show_tokens(animal_text)\n",
    "    model.run_with_hooks(animal_text, fwd_hooks=[(f'blocks.{layer}.mlp.hook_post', running_total_hook)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the dataset example that most activates a given neuron\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# We focus on neuron 13 in layer 5\n",
    "model.reset_hooks(clear_contexts=True)\n",
    "animal_texts = ['The dog was green', 'The cat was blue', 'The squid was magenta', 'The blobfish was grey']\n",
    "layer = 5\n",
    "neuron_index = 13\n",
    "def best_act_hook(neuron_acts, hook, text):\n",
    "    if 'best' not in hook.ctx:\n",
    "        hook.ctx['best']=-1e3\n",
    "    print('Neuron acts:', neuron_acts[0, :, neuron_index])\n",
    "    if hook.ctx['best']<neuron_acts[0, :, neuron_index].max():\n",
    "        print(f'Updating best act from {hook.ctx[\"best\"]} to {neuron_acts[0, :, neuron_index].max().item()}')\n",
    "        hook.ctx['best'] = neuron_acts[0, :, neuron_index].max().item()\n",
    "        hook.ctx['text'] = text\n",
    "\n",
    "for animal_text in animal_texts:\n",
    "    (show_tokens(animal_text))\n",
    "    # Use partial to give the hook access to the relevant text\n",
    "    model.run_with_hooks(animal_text, fwd_hooks=[(f'blocks.{layer}.mlp.hook_post', partial(best_act_hook, text=animal_text))])\n",
    "print()\n",
    "print('Maximally activating dataset example:', model.hook_dict[f'blocks.{layer}.mlp.hook_post'].ctx['text'])\n",
    "model.reset_hooks(clear_contexts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Fancier Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking for heads that mostly attend to the previous token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "long_text = 'Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.'\n",
    "print('Long text:', long_text)\n",
    "# We first cache attention patterns\n",
    "attn_cache = {}\n",
    "def cache_attn(attn, hook):\n",
    "    attn_cache[hook.name]=attn\n",
    "logits = model.run_with_hooks(long_text, fwd_hooks=[(filter_hook_attn, cache_attn)])\n",
    "\n",
    "# We then go through the cache and find the average attention paid to previous tokens\n",
    "prev_token_scores = np.zeros((model.cfg.n_layers, model.cfg.n_heads))\n",
    "for layer in range(model.cfg.n_layers):\n",
    "    for head in range(model.cfg.n_heads):\n",
    "        attn = attn_cache[f\"blocks.{layer}.attn.hook_attn\"][0, head]\n",
    "        prev_token_scores[layer, head]=attn.diag(-1).mean().item()\n",
    "\n",
    "px.imshow(prev_token_scores, \n",
    "          x=[f'Head {hi}' for hi in range(model.cfg.n_heads)], \n",
    "          y=[f'Layer {i}' for i in range(model.cfg.n_layers)], \n",
    "          title='Prev Token Scores', \n",
    "          color_continuous_scale='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ROME style](https://rome.baulab.info/) patching for causal tracing - we have two runs with two different prompts and different answers, eg \"Steve Jobs founded\" -> \" Apple\" and \"Bill Gates founded\" -> \" Microsoft\". We patch parts of the layer outputs or residual stream from specific tokens and positions and see which patches significantly shift the answer from \" Apple\" to \" Microsoft\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "prompt_1 = 'Bill Gates founded'\n",
    "response_1 = ' Microsoft'\n",
    "logit_index_1 = model.to_tokens(response_1)[0][-1]\n",
    "(show_tokens(prompt_1))\n",
    "prompt_2 = 'Steve Jobs founded'\n",
    "response_2 = ' Apple'\n",
    "logit_index_2 = model.to_tokens(response_2)[0][-1]\n",
    "show_tokens(prompt_2)\n",
    "\n",
    "model.reset_hooks()\n",
    "uncorrupted_cache = {}\n",
    "model.cache_all(uncorrupted_cache)\n",
    "logits_1 = model(prompt_1)\n",
    "model.reset_hooks()\n",
    "\n",
    "uncorrupted_logits = model(prompt_2)\n",
    "uncorrupted_log_probs = F.log_softmax(uncorrupted_logits, dim=-1)\n",
    "print('Uncorrupted log prob for', response_1, uncorrupted_log_probs[0, -1, logit_index_1].item())\n",
    "print('Uncorrupted log prob for', response_2, uncorrupted_log_probs[0, -1, logit_index_2].item())\n",
    "\n",
    "# Patch the residual stream from the Bill Gates run to the Steve Jobs run\n",
    "# at the Jobs/Gates token, at the start of layer 7\n",
    "layer = 7\n",
    "position = 1\n",
    "\n",
    "def patch_resid_pre(resid_pre, hook):\n",
    "    uncorrupted_resid_pre = uncorrupted_cache[hook.name]\n",
    "    # Move things on the Jobs/Gates token\n",
    "    resid_pre[:, position] = uncorrupted_resid_pre[:, position]\n",
    "    return resid_pre\n",
    "\n",
    "corrupted_logits = model.run_with_hooks(prompt_2, \n",
    "                    fwd_hooks=[(f'blocks.{layer}.hook_resid_pre', patch_resid_pre)])\n",
    "corrupted_log_probs = F.log_softmax(corrupted_logits, dim=-1)\n",
    "print('Corrupted (Residual) log prob for', response_1, corrupted_log_probs[0, -1, logit_index_1].item())\n",
    "print('Corrupted (Residual) log prob for', response_2, corrupted_log_probs[0, -1, logit_index_2].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also patch the outputs of MLP layers 0 to 7 on the Gates/Jobs token - this time, rather than giving a hook name, we give a Boolean function that filters for the names of those hooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "layer_start = 0\n",
    "layer_end = 7\n",
    "\n",
    "def patch_mlp_post(mlp_post, hook):\n",
    "    return uncorrupted_cache[hook.name]\n",
    "\n",
    "def filter_middle_mlps(name):\n",
    "    split_name = name.split('.')\n",
    "    if split_name[-1]=='hook_post':\n",
    "        layer = int(split_name[1])\n",
    "        return (layer_start<=layer<layer_end)\n",
    "    return False\n",
    "\n",
    "corrupted_logits = model.run_with_hooks(prompt_2, \n",
    "                    fwd_hooks=[(filter_middle_mlps, patch_mlp_post)])\n",
    "corrupted_log_probs = F.log_softmax(corrupted_logits, dim=-1)\n",
    "print('Corrupted (MLP) log prob for', response_1, corrupted_log_probs[0, -1, logit_index_1].item())\n",
    "print('Corrupted (MLP) log prob for', response_2, corrupted_log_probs[0, -1, logit_index_2].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking for [induction heads](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html), by feeding in a random sequence of tokens repeated twice and looking for heads that attend from a second copy of a token to the token just after the first copy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "seq_len = 100\n",
    "rand_tokens = torch.randint(1000, 10000, (4, seq_len))\n",
    "rand_tokens_repeat = einops.repeat(rand_tokens, 'batch pos -> batch (2 pos)')\n",
    "if torch.cuda.is_available():\n",
    "    rand_tokens_repeat = rand_tokens_repeat.cuda()\n",
    "\n",
    "induction_scores_array = np.zeros((model.cfg.n_layers, model.cfg.n_heads))\n",
    "def calc_induction_score(attn_pattern, hook):\n",
    "    # Pattern has shape [batch, index, query_pos, key_pos]\n",
    "    induction_stripe = attn_pattern.diagonal(1-seq_len, dim1=-2, dim2=-1)\n",
    "    induction_scores = einops.reduce(induction_stripe, 'batch index pos -> index', 'mean')\n",
    "    # Store the scores in a common array\n",
    "    induction_scores_array[hook.layer()] = induction_scores.detach().cpu().numpy()\n",
    "    \n",
    "def filter_attn_hooks(hook_name):\n",
    "    split_name = hook_name.split('.')\n",
    "    return split_name[-1]=='hook_attn'\n",
    "\n",
    "induction_logits = model.run_with_hooks(rand_tokens_repeat, fwd_hooks=[(filter_attn_hooks, calc_induction_score)])\n",
    "px.imshow(induction_scores_array, labels={'y':'Layer', 'x':'Head'}, color_continuous_scale='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validation:** We can ablate the top few heads by this metric, and show that performance goes down substantially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "induction_logits = model(rand_tokens_repeat)\n",
    "induction_log_probs = F.log_softmax(induction_logits, dim=-1)\n",
    "induction_pred_log_probs = torch.gather(induction_log_probs[:, :-1], -1, rand_tokens_repeat[:, 1:, None])[..., 0]\n",
    "print('Original loss on repeated sequence:', induction_pred_log_probs[:, seq_len:].mean())\n",
    "\n",
    "# Mask out the heads with a high induction score\n",
    "attn_head_mask = induction_scores_array>0.8\n",
    "\n",
    "def prune_attn_heads(value, hook):\n",
    "    # Value has shape [batch, pos, index, d_head]\n",
    "    mask = attn_head_mask[hook.layer()]\n",
    "    value[:, :, mask] = 0.\n",
    "    return value\n",
    "\n",
    "def filter_value_hooks(name):\n",
    "    return name.split('.')[-1]=='hook_v'\n",
    "\n",
    "ablated_logits = model.run_with_hooks(rand_tokens_repeat, fwd_hooks=[(filter_value_hooks, prune_attn_heads)])\n",
    "ablated_log_probs = F.log_softmax(ablated_logits, dim=-1)\n",
    "ablated_pred_log_probs = torch.gather(ablated_log_probs[:, :-1], -1, rand_tokens_repeat[:, 1:, None])[..., 0]\n",
    "print('Loss on repeated sequence without induction heads:', ablated_pred_log_probs[:, seq_len:].mean())\n",
    "\n",
    "px.imshow(attn_head_mask, labels={'y':'Layer', 'x':'Head'}, color_continuous_scale='Blues', title='Mask').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ablation experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide a wrapper to facilitate ablations experiment. \n",
    "\n",
    "An `EasyAblation` object is the combinaison of:\n",
    "* The `EasyTransformer` model to be ablated\n",
    "* An `AblationConfig` object that store all the parameters of the ablation \n",
    "* An `ExperimentMetric` object that define how we will mesure the effect of the ablation. This can be the loss on a given dataset or the attention score of a precise head.\n",
    "\n",
    "Here, we defined a metric function that takes in inputs the model and the dataset and output a tensor. You can use `model.run_with_hook()` in the metric function, by you *have* to use the option `reset_hooks_start=False`, else the ablation hooks will be ignored. \n",
    "\n",
    "You can specify the `target_module` from \"mlp\", \"attn_layer\", \"attn_head\". \n",
    "If you chose \"attn_head\" you can define wich part of the head computation to ablate (\"z\", \"q\", \"v\", \"k\", \"attn\", \"attn_scores\") \n",
    "\n",
    "The supported ablation types are `mean`, `zero`, `neg` and `custom`. For mean ablations, you can specify a `mean_dataset` in the config that can be different to the one used in the metric. \n",
    "\n",
    "The `verbose` option prints all the experiment parameters before running the ablations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def induction_loss(model, dataset):\n",
    "    induction_logits = model(dataset)\n",
    "    induction_log_probs = F.log_softmax(induction_logits, dim=-1)\n",
    "    induction_pred_log_probs = torch.gather(induction_log_probs[:, :-1], -1, rand_tokens_repeat[:, 1:, None])[..., 0]\n",
    "    return induction_pred_log_probs[:, seq_len:].mean()\n",
    "\n",
    "metric = ExperimentMetric(metric=induction_loss, dataset=rand_tokens_repeat, relative_metric=True)\n",
    "config = AblationConfig(abl_type=\"zero\", target_module=\"attn_head\",head_circuit=\"z\",  cache_means=True, verbose=True)\n",
    "abl = EasyAblation(model, config, metric)\n",
    "result = abl.run_ablation()\n",
    "\n",
    "px.imshow(result, labels={'y':'Layer', 'x':'Head'}, color_continuous_scale='Blues', title='Induction Score Variation after Ablation').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use the `EasyAblation` object to generate ablations hook, without using it to run ablations. This is useful when you want to ablate several heads at once. The ablations hooks generated will respect the configuration you used to generate the `EasyAblation` object.\n",
    "\n",
    "Here we can reproduce the revious results where we ablate the induction heads by replacing their activation by zero. You can notice that the combined effect of the ablations of the 5 heads is much greater than the sum of their individual effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "model.reset_hooks()\n",
    "for (l,h) in [(5,1), (5,5), (6,9), (7,2), (7,10)]:\n",
    "    hook_name, hook = abl.get_hook(l,h)\n",
    "    model.add_hook(hook_name, hook)\n",
    "print(f\"Loss on the repeated random token after zero-ablations of the induction heads {induction_loss(model, rand_tokens_repeat)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, replacing the activation by zero is quite a weird thing to do as this could be really far from the baseline activation depending on the head. To fix this, we can instead replace the activation by its mean on the dataset. All the effect related to a particular sample will be washed out but the global contribution will be kept. The drop is still significant but not as much as zero ablation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "mean_abl_config = AblationConfig(abl_type=\"mean\", target_module=\"attn_head\",head_circuit=\"z\", cache_means=True)\n",
    "mean_abl = EasyAblation(model, mean_abl_config, metric)\n",
    "model.reset_hooks()\n",
    "for (l,h) in [(5,1), (5,5), (6,9), (7,2), (7,10)]:\n",
    "    hook_name, hook = mean_abl.get_hook(l,h)\n",
    "    model.add_hook(hook_name, hook)\n",
    "    \n",
    "print(f\"Loss on the repeated random token after mean-ablations of the induction heads {induction_loss(model, rand_tokens_repeat)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom ablation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using `abl_type=\"custom\"` you can specify an arbitrary function `custom_abl_fn`. It has to take as input the normal output of the module, its mean activation, the hook object and output a tensor of the same shape as the normal output.\n",
    "\n",
    "The mean as the same shape as the normal output, its constant along the batch_size dimension. \n",
    "\n",
    "If `target_module=\"attn_head\"` the output is for a given head, its shape would be `(batch, seq_len, head_dim)` except for attention score and attention pattern : `(batch,seq_len, seq_len)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example below, we take the symetric of the activation with respect to its mean. It has for effect to reverse the contribution of the head without going too far in out of distribution activation space (as naively flipping the sign would do)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def sym_mean(z, mean, hook): \n",
    "    return mean-(z-mean)\n",
    "\n",
    "metric = ExperimentMetric(induction_loss, rand_tokens_repeat, relative_metric=True)\n",
    "config = AblationConfig(abl_type=\"custom\",abl_fn=sym_mean, target_module=\"attn_head\",head_circuit=\"v\",  cache_means=True, verbose=False)\n",
    "abl = EasyAblation(model, config, metric)\n",
    "result = abl.run_ablation()\n",
    "fig = px.imshow(result, labels={'y':'Layer', 'x':'Head'}, color_continuous_scale='Blues', title='Induction Score Variation after Custom Ablation')\n",
    "# fig.update_xtick()\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patching experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also run patching experiment were we take activation from a source dataset and copy them in the model while processing the target dataset. We can then mesure wich module causes the model to change its output.\n",
    "Similarly to the `EasyAblation`, we use an `EasyPatching` object that depends on a `PatchingConfig` and an `ExperimentMetric`.\n",
    "\n",
    "In the example bellow we can locate which head influences the predicted next token at \"founded\" to got from \"Microsoft\" to \"Apple\" if the sentence is \"Bill Gates founded\". It seems that head 10.0 plays the major role here, by only changing its activation we can make the next token prediction switch from \"Microsoft\" to \"Apple\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from easy_transformer.experiments import EasyPatching, PatchingConfig\n",
    "\n",
    "source_facts = [\"Steve Jobs founded Apple\", \"Bill Gates founded Microsoft\"]\n",
    "target_facts = [\"Bill Gates founded Microsoft\", \"Steve Jobs founded Apple\"]\n",
    "\n",
    "source_labels = [ \" Apple\", \" Microsoft\"]\n",
    "target_labels = [ \" Microsoft\",\" Apple\"]\n",
    "\n",
    "source_logits = model.to_tokens(source_labels).squeeze()\n",
    "target_logits = model.to_tokens(target_labels).squeeze()\n",
    "\n",
    "tokens_pos = [2,2] #the position of \"founded\" in the target sentences, where to get the next token prediction\n",
    "\n",
    "def fact_transfer_score(model, target_dataset):\n",
    "    logits = model(target_dataset)\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    logit_diff = (log_probs[torch.arange(len(target_logits)),tokens_pos,target_logits] - #logit target - logit source (positive by default)\n",
    "                  log_probs[torch.arange(len(source_logits)),tokens_pos,source_logits])\n",
    "\n",
    "    return logit_diff.mean() \n",
    "\n",
    "metric = ExperimentMetric(fact_transfer_score, target_facts, relative_metric=False)\n",
    "config = PatchingConfig(source_dataset = source_facts,target_dataset = target_facts, target_module=\"attn_head\",head_circuit=\"v\",  cache_act=True, verbose=False)\n",
    "patching = EasyPatching(model, config, metric)\n",
    "result = patching.run_patching()\n",
    "px.imshow(result, labels={'y':'Layer', 'x':'Head'}, color_continuous_scale='Blues', title='Absolute Log Logit Prob Difference After Patching').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can be more precise and patch at only certain token position. For that, we can use custom patching functions. Below we show that patching at the last name position is enough to recover the previous plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def patch_last_name(z, source_act, hook):\n",
    "    z[:, 1, :] = source_act[:, 1, :] #we patch at the token of the last name \n",
    "    return z\n",
    "\n",
    "config = PatchingConfig(\n",
    "    source_dataset=source_facts,\n",
    "    target_dataset=target_facts,\n",
    "    patch_fn=patch_last_name,\n",
    "    target_module=\"attn_head\",\n",
    "    head_circuit=\"v\",\n",
    "    cache_act=True,\n",
    "    verbose=False,\n",
    ")\n",
    "patching = EasyPatching(model, config, metric)\n",
    "result = patching.run_patching()\n",
    "px.imshow(result, labels={'y':'Layer', 'x':'Head'}, color_continuous_scale='Blues', title='Log Logit Prob difference after Patching').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Loading Checkpointed Models\n",
    "Researchers at the Stanford Center for Research on Foundation Models kindly [created and open sourced 5 training runs of GPT-2 Small and GPT-2 Medium](https://huggingface.co/stanford-crfm), with 600 checkpoints taken during training. These can be loaded in via the same interface as above\n",
    "\n",
    "These are called `stanford-gpt2-small-A`, (with `small` or `medium` and `A`, `B`, `C`, `D`, `E` as the possible options)\n",
    "\n",
    "You can see the available checkpoints [here](https://huggingface.co/stanford-crfm/alias-gpt2-small-x21/tree/main) (each checkpoint has a separate Git branch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model loading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "checkpointed_model = EasyTransformer('stanford-gpt2-small-A', checkpoint=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from easy_transformer.EasyTransformer import STANFORD_CRFM_CHECKPOINTS\n",
    "print(\"Available checkpoints - it's long, so toggle the flag to print\")\n",
    "print_checkpoints = False #@param [False, True]\n",
    "if print_checkpoints:\n",
    "    print(STANFORD_CRFM_CHECKPOINTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Looking for induction heads\n",
    "We can use this to analyse whether models contain induction heads during training (by checking whether they can predict repeated sequences of random tokens), and can see something of a phase change early in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "plps = {}\n",
    "tokens = torch.randint(1000, 20000, (1, 100))\n",
    "tokens = torch.concat([tokens, tokens], axis=1)\n",
    "for check in [1000, 2500, 5000]:\n",
    "    model = EasyTransformer('stanford-gpt2-small-E', checkpoint=check)\n",
    "    logits = model(tokens)\n",
    "    log_probs = F.log_softmax(logits, dim=-1)\n",
    "    plp = torch.gather(log_probs[:, :-1], -1, tokens[:, 1:, None])[0, :, 0]\n",
    "    plps[check] = plp.detach().cpu().numpy()\n",
    "px.line(plps).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Visualisations\n",
    "\n",
    "Visualisations are an extremely important tool for doing good interpretability work - fundamentally, neural networks are complex, high-dimensional objects and we want to understand how to decompose them and understand them. \n",
    "\n",
    "It's important to get as close as you can to the ground truth of what's really going on, and easy to trick yourself and shoot yourself in the foot, so it's hard to get by with just things like summary statistics - which is where data visualisation techniques come in!\n",
    "\n",
    "Visualisations are not currently planned to be part of EasyTransformer, but Anthropic released a very rough library for doing nice and transformer-relevant visualisations within Python called PySvelte, a slightly less rough fork can be found here: https://github.com/neelnanda-io/pysvelte (Note - this library is under active development by a friend of mine and will hopefully be much more stable and usable by October-ish - I don't recommend putting significant effort into understanding how to edit and write your own components unless you already have a bunch of webdev experience). Credit to Oliver Balfour for helping me figure out how to get it to work!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install the right version of node - this is needed to install Svelte which is used to build components\n",
    "# v16, an older version, seems to work more reliably than v18 on the systems I've tried it on.\n",
    "!curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -; sudo apt-get install -y nodejs\n",
    "# Get an up-to-date PySvelte, deleting old versions if present\n",
    "!touch PySvelte\n",
    "!rm -r PySvelte\n",
    "!git clone https://github.com/neelnanda-io/PySvelte.git\n",
    "import sys\n",
    "sys.path.append('/content/PySvelte')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import pysvelte\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Running pysvelte the first time will re-compile the Svelte UI code, which might take a while\n",
    "pysvelte.Hello(name='World')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Visualising Attention Patterns\n",
    "\n",
    "A component to visualise attention patterns over some text. This is a particularly hard problem, as attention patterns are rank 3 tensors - with a destination_pos, source_pos and num_heads dimension. \n",
    "\n",
    "This plots the attention pattern as a dest_pos x source_pos grid in the top left, showing the average across the heads, and along the bottom shows the tokenized text, each token highlighted with the average attention paid to it. \n",
    "\n",
    "Each head gets a colour, and by default the colours are averaged, but we can hover over or click on a head to focus on just that head's colour.\n",
    "\n",
    "If we hover over or click on a token, it instead shows that the shading over other tokens according to attention paid from that token to them.\n",
    "\n",
    "There's a toggle to flip the token view to show tokens attending TO the current token (ie, from later in the sequence attending back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def text_to_token_strings(text):\n",
    "    # Extremely hacky function to convert text into a list of each token (as text, not as a token index)\n",
    "    return model.tokenizer.batch_decode(model.tokenizer.encode(text), clean_up_tokenization_spaces=False)\n",
    "text_to_token_strings('1234567890')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "model = EasyTransformer('gpt2')\n",
    "model.reset_hooks()\n",
    "vis_cache = {}\n",
    "model.cache_all(vis_cache)\n",
    "vis_text = \"Help, I live in three dimensions but need to interact with models with too many dimensions!! Help, I live in three dimensions but need to interact with models with too many dimensions!\"\n",
    "logits = model(vis_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "layer = 5\n",
    "attn_pattern = einops.rearrange(vis_cache[f'blocks.{layer}.attn.hook_attn'][0], \n",
    "                                \"num_heads dest_pos src_pos -> dest_pos src_pos num_heads\") # Indexing into shape [batch, n_heads, dest_pos, src_pos]\n",
    "\n",
    "tokenized_text = text_to_token_strings(vis_text)\n",
    "html_object = pysvelte.AttentionMulti(tokens=tokenized_text, attention=attn_pattern, head_labels=None)\n",
    "html_object.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Earlier we saw that layer 5 contained some induction heads - can you figure out what they are from the above diagram? \n",
    "\n",
    "(Hint: What should the attention pattern visualised as a grid for each head look like?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Visualising Neuron Activations\n",
    "\n",
    "We can also plot neuron activations over text - we input a list of the token in the text, and 1D array of activations, one per token. \n",
    "\n",
    "Positive activations are coloured green and negative are red, and it's automatically normalised to be in [-1, 1] (the max and min are printed at the top)\n",
    "\n",
    "(This currently just supports activations for a single neuron, though wouldn't be too hard to extend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "layer = 7\n",
    "neuron = 124\n",
    "neuron_activations = vis_cache[f\"blocks.{layer}.mlp.hook_post\"][0, :, neuron] # Indexing into shape [batch, pos, d_mlp]\n",
    "\n",
    "html_object = pysvelte.TextSingle(tokens=tokenized_text, activations=neuron_activations, neuron_name='Test neuron')\n",
    "html_object.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Training an Algorithmic Model\n",
    "\n",
    "EasyTransformer also supports passing in custom config and initialising weights to create your own model. This isn't optimised for performance, so is likely best for training small LMs or small transformers for algorithmic tasks.\n",
    "\n",
    "We demonstrate training a small model to predict a string of constant tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "training_cfg = EasyTransformerConfig(\n",
    "    d_model = 32,\n",
    "    d_head = 16,\n",
    "    n_heads = 2,\n",
    "    d_mlp = 128,\n",
    "    n_layers=1,\n",
    "    n_ctx = 20,\n",
    "    act_fn='solu_ln',\n",
    "    d_vocab=100,\n",
    "    normalization_type='LN',\n",
    "    )\n",
    "tiny_model = EasyTransformer.from_config(training_cfg).to(device)\n",
    "tiny_optimizer = torch.optim.Adam(tiny_model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "for epoch in tqdm.tqdm(range(301)):\n",
    "    random_int = torch.randint(0, 100, (20,))\n",
    "    batch = einops.repeat(random_int, 'batch -> batch pos', pos=tiny_model.cfg.n_ctx).to(device)\n",
    "    loss = tiny_model(batch, return_type='loss')\n",
    "    loss.backward()\n",
    "    tiny_optimizer.step()\n",
    "    tiny_optimizer.zero_grad()\n",
    "    if epoch%100 == 0:\n",
    "        print(f\"Epoch: {epoch}. Loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "tiny_cache = {}\n",
    "tiny_model.cache_all(tiny_cache)\n",
    "loss = tiny_model(batch, return_type='loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Training a Language Model\n",
    "\n",
    "Though EasyTransformer is not designed for high-performance model training, we provide some utilities for training small language models.\n",
    "\n",
    "See train.py for an example training script, the following is how to use it for a simple training task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "micro_gpt_cfg = EasyTransformerConfig(\n",
    "    d_model = 64,\n",
    "    d_head = 32,\n",
    "    n_heads = 2,\n",
    "    d_mlp = 256,\n",
    "    n_layers=3,\n",
    "    n_ctx = 512,\n",
    "    act_fn='gelu_new',\n",
    "    normalization_type='LN',\n",
    "    tokenizer_name='gpt2',\n",
    "    )\n",
    "micro_gpt = EasyTransformer.from_config(micro_gpt_cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We download 10K samples of OpenWebText (kindly provided by a user on HuggingFace), and use a utility to tokenize them, concatenate them (separated by EOS tokens), and reshape them into batches of size n_ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset openwebtext-10k (/workspace/cache/stas___openwebtext-10k/plain_text/1.0.0/3a8df094c671b4cb63ed0b41f40fb3bd855e9ce2e3765e5df50abcdfb5ec144b)\n",
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39d656126917426bb23116ee63951832",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69e8b45975d14cd981b8b54e9f7f6a38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ddb2f9df4db416faad36ce6d08f6609",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f02936d16094d97b3c5ed807f4f8b52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (56043 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (59222 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (56546 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (63008 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset = datasets.load_dataset(\"stas/openwebtext-10k\", split=\"train\")\n",
    "dataset = easy_transformer.utils.tokenize_and_concatenate(dataset, micro_gpt.tokenizer, max_length=micro_gpt.cfg.n_ctx, add_bos_token=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, we train our tiny model for 1000 steps, of batch size 2, with AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5099fb92cdb54fa482b06bf9dca31ace",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "759f265f8d43418292a2f213822a4a12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Samples 2 Step 0 Loss 10.962458610534668\n",
      "Epoch 1 Samples 102 Step 50 Loss 8.05038833618164\n",
      "Epoch 1 Samples 202 Step 100 Loss 6.9176506996154785\n",
      "Epoch 1 Samples 302 Step 150 Loss 7.691259860992432\n",
      "Epoch 1 Samples 402 Step 200 Loss 6.860898017883301\n",
      "Epoch 1 Samples 502 Step 250 Loss 7.120456218719482\n",
      "Epoch 1 Samples 602 Step 300 Loss 7.272021293640137\n",
      "Epoch 1 Samples 702 Step 350 Loss 7.017391681671143\n",
      "Epoch 1 Samples 802 Step 400 Loss 7.513564586639404\n",
      "Epoch 1 Samples 902 Step 450 Loss 6.70914363861084\n",
      "Epoch 1 Samples 1002 Step 500 Loss 6.509733200073242\n"
     ]
    }
   ],
   "source": [
    "\n",
    "training_cfg = easy_transformer.train.EasyTransformerTrainConfig(\n",
    "    num_epochs = 1,\n",
    "    batch_size = 2,\n",
    "    weight_decay = 0.01,\n",
    "    optimizer_name = 'AdamW',\n",
    "    max_steps = 500,\n",
    ")\n",
    "micro_gpt = easy_transformer.train.train(micro_gpt, training_cfg, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('base')",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
